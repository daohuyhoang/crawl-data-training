from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import pandas as pd

import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import time
import re
import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def is_junk(text):
    # 1. L·ªçc c√°c d√≤ng ch·ªâ c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát ho·∫∑c qu√° nhi·ªÅu k√Ω t·ª± l·∫°
    if not text or len(text.strip()) < 10:
        return True
    
    # K√Ω t·ª± l·∫°/spam nh∆∞ ÏõÉ‚û´, ‚ô´‚ôØ, [r]
    junk_patterns = [r'ÏõÉ‚û´', r'‚ô´‚ôØ', r'\[r\]', r'‚ôó', r'‚û´']
    for p in junk_patterns:
        if re.search(p, text):
            return True
            
    # 2. L·ªçc qu·∫£ng c√°o (s·ªë ƒëi·ªán tho·∫°i, Hotline, Zalo)
    if re.search(r'0\d{9,10}', text) or "Hotline" in text or "Zalo" in text or "MI·ªÑN PH√ç" in text:
        return True
        
    # 3. L·ªçc c√°c c√¢u c√≥ qu√° nhi·ªÅu k√Ω t·ª± ƒë·∫∑c bi·ªát (> 30% n·ªôi dung)
    special_chars = len(re.sub(r'[\w\s,.]', '', text))
    if special_chars / len(text) > 0.3:
        return True
        
    return False

def crawl_fb_comments(post_url, max_comments=5000):
    chrome_options = Options()
    chrome_options.add_argument("--disable-notifications")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    comments_data = set()
    
    try:
        driver.get(post_url)
        print("--- ƒêang t·∫£i trang... ---")
        print("TIPS: B·∫°n n√™n ƒêƒÇNG NH·∫¨P ƒë·ªÉ l·∫•y ƒë∆∞·ª£c nhi·ªÅu b√¨nh lu·∫≠n h∆°n.")
        print("B·∫°n c√≥ 30 gi√¢y ƒë·ªÉ chu·∫©n b·ªã...")
        time.sleep(30)
        
        # Th·ª≠ chuy·ªÉn sang ch·∫ø ƒë·ªô "T·∫•t c·∫£ b√¨nh lu·∫≠n"
        try:
            filters = [
                "//span[contains(text(),'Ph√π h·ª£p nh·∫•t') or contains(text(),'Most relevant')]",
                "//div[@role='button']//span[contains(text(), 'B√¨nh lu·∫≠n h√†ng ƒë·∫ßu')]",
                "//div[@role='button']//i[contains(@class, 'x1b00660')]"
            ]
            for f in filters:
                try:
                    btn = driver.find_element(By.XPATH, f)
                    driver.execute_script("arguments[0].click();", btn)
                    time.sleep(2)
                    all_opt = WebDriverWait(driver, 5).until(
                        EC.element_to_be_clickable((By.XPATH, "//span[contains(text(),'T·∫•t c·∫£ b√¨nh lu·∫≠n') or contains(text(),'All comments')]"))
                    )
                    all_opt.click()
                    print("=> ƒê√£ chuy·ªÉn sang ch·∫ø ƒë·ªô: T·∫•t c·∫£ b√¨nh lu·∫≠n")
                    time.sleep(3)
                    break
                except:
                    continue
        except:
            pass

        last_count = 0
        no_new_retry = 0
        
        while len(comments_data) < max_comments:
            # 1. Click "Xem th√™m"
            see_more_xpaths = [
                "//span[contains(text(), 'Xem th√™m b√¨nh lu·∫≠n')]",
                "//span[contains(text(), 'View more comments')]",
                "//span[contains(text(), 'Xem c√°c b√¨nh lu·∫≠n tr∆∞·ªõc')]",
                "//span[contains(text(), 'View previous comments')]",
                "//span[contains(text(), 'Xem th√™m tr·∫£ l·ªùi')]",
                "//div[contains(text(), 'replies')]"
            ]
            
            for xpath in see_more_xpaths:
                btns = driver.find_elements(By.XPATH, xpath)
                for b in btns:
                    try:
                        driver.execute_script("arguments[0].click();", b)
                    except:
                        continue

            # 2. Cu·ªôn d·∫ßn
            driver.execute_script("window.scrollBy(0, 1500);")
            time.sleep(2)

            # 3. L·∫•y d·ªØ li·ªáu
            articles = driver.find_elements(By.CSS_SELECTOR, "div[role='article']")
            for art in articles:
                try:
                    comment_parts = art.find_elements(By.CSS_SELECTOR, "div[dir='auto']")
                    for p in comment_parts:
                        text = p.text
                        if text and not is_junk(text):
                            comments_data.add(text.strip())
                except:
                    continue
                
            current_count = len(comments_data)
            print(f"ƒêang l·∫•y b√¨nh lu·∫≠n m·ªõi...")
            
            if current_count == last_count:
                no_new_retry += 1
                if no_new_retry > 8: # TƒÉng s·ªë l·∫ßn th·ª≠ l√™n cho ch·∫Øc
                    print("=> H·∫øt b√¨nh lu·∫≠n c√≥ th·ªÉ l·∫•y.")
                    break
            else:
                no_new_retry = 0
                
            last_count = current_count
            
    except Exception as e:
        print(f"L·ªói: {e}")
    finally:
        # L∆∞u v√†o file (Ch·∫ø ƒë·ªô APPEND - c·ªông d·ªìn)
        save_file = 'crawled_fb.csv'
        new_df = pd.DataFrame(list(comments_data), columns=['text'])
        # Kh√¥ng g√°n nh√£n ·ªü ƒë√¢y - s·∫Ω g√°n b·∫±ng BERT sau
        
        if not os.path.isfile(save_file):
            new_df.to_csv(save_file, index=False, encoding='utf-8-sig')
        else:
            # ƒê·ªçc file c≈© ƒë·ªÉ tr√°nh tr√πng l·∫∑p khi append
            old_df = pd.read_csv(save_file)
            combined_df = pd.concat([old_df, new_df]).drop_duplicates(subset=['text'])
            combined_df.to_csv(save_file, index=False, encoding='utf-8-sig')
            
        print(f"\n--- HO√ÄN TH√ÄNH ---")
        print(f"T·ªïng s·ªë d·ªØ li·ªáu m·ªõi trong {save_file}: {len(pd.read_csv(save_file))}")
        print(f"\nüí° Ti·∫øp theo: Ch·∫°y 'python auto_label_bert.py' ƒë·ªÉ g√°n nh√£n t·ª± ƒë·ªông!")
        driver.quit()

if __name__ == "__main__":
    print("--- Facebook Scraper Pro (Append Mode & Junk Filter) ---")
    url_input = input("D√°n link Facebook b·∫°n mu·ªën c√†o s·∫°ch: ")
    if url_input:
        crawl_fb_comments(url_input)
    else:
        print("Vui l√≤ng nh·∫≠p URL.")

